{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBTI_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(4096, 16)\n",
    "#         self.hidden2 = nn.Linear(1024, 512)\n",
    "#         self.hidden3 = nn.Linear(512, 128)\n",
    "#         self.output = nn.Linear(128, 16)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden1(x)\n",
    "        x = self.sigmoid(x)\n",
    "#         x = self.hidden2(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         x = self.hidden3(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBTI_NN(\n",
       "  (hidden1): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MBTI_NN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/amoghmishra23/appledore/MBTI-Personality-Detection/data/mbti_comments/mbti_comments_cleaned.csv',chunksize=1000)\n",
    "# embedding = load('output/embedding_whole.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for chunk in df:\n",
    "    labels = labels + chunk['type'].tolist()\n",
    "\n",
    "label_encoding = { \"istj\":1, \"istp\":2, \"isfj\":3, \"isfp\":4, \"infj\":5, \"infp\":6, \"intj\":7, \"intp\":8, \"estp\":9, \"estj\":10, \"esfp\":11, \"esfj\":12, \"enfp\":13, \"enfj\":14, \"entp\":15, \"entj\":16 }\n",
    "label_enc = []\n",
    "for label in labels:\n",
    "    label_enc.append(label_encoding[label]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/embedding_infersent.txt','r') as fr:\n",
    "    embedding = fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = list(map(eval, embedding)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(embedding,label_enc,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 14,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 11,\n",
       " 12,\n",
       " 4,\n",
       " 7,\n",
       " 14,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 10,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 14,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 12,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 15,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 12,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 13,\n",
       " 14,\n",
       " 4,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 14,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 14,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 13,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 15,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 10,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 12,\n",
       " 5,\n",
       " 12,\n",
       " 14,\n",
       " 6,\n",
       " 7,\n",
       " 14,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 15,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 15,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 14,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 15,\n",
       " 12,\n",
       " 12,\n",
       " 5,\n",
       " 12,\n",
       " 6,\n",
       " 7,\n",
       " 13,\n",
       " 14,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 15,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 7,\n",
       " 13,\n",
       " 4,\n",
       " 4,\n",
       " 12,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 15,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 15,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 15,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 14,\n",
       " 7,\n",
       " 14,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 15,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 13,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 14,\n",
       " 7,\n",
       " 14,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 15,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 14,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 14,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 13,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 13,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 13,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 15,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 14,\n",
       " 12,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 7,\n",
       " 15,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 12,\n",
       " 6,\n",
       " 0,\n",
       " 14,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 15,\n",
       " 5,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 12,\n",
       " 5,\n",
       " 13,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 12,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 12,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 13,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 14,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 13,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 12,\n",
       " 6,\n",
       " 12,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 12,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 15,\n",
       " 7,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 14,\n",
       " 3,\n",
       " 14,\n",
       " 13,\n",
       " 6,\n",
       " 5,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 15,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 14,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 13,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 12,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 14,\n",
       " 7,\n",
       " 1,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 15,\n",
       " 2,\n",
       " 7,\n",
       " 14,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 15,\n",
       " 4,\n",
       " 15,\n",
       " 5,\n",
       " 6,\n",
       " 14,\n",
       " 14,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 15,\n",
       " 6,\n",
       " 2,\n",
       " 13,\n",
       " 12,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 15,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 14,\n",
       " 6,\n",
       " 7,\n",
       " 10,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 14,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 12,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 15,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 6,\n",
       " 10,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 14,\n",
       " 6,\n",
       " 11,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 14,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 13,\n",
       " 14,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 14,\n",
       " 15,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 14,\n",
       " 12,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 14,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 13,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 13,\n",
       " 12,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 7,\n",
       " 15,\n",
       " 14,\n",
       " 7,\n",
       " 4,\n",
       " 12,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 13,\n",
       " 12,\n",
       " 7,\n",
       " 6,\n",
       " 14,\n",
       " 15,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 15,\n",
       " 7,\n",
       " 14,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 12,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 15,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 15,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 12,\n",
       " 6,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 14,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 14,\n",
       " 4,\n",
       " 15,\n",
       " 6,\n",
       " 4,\n",
       " 14,\n",
       " 6,\n",
       " 4,\n",
       " 13,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 14,\n",
       " 5,\n",
       " 6,\n",
       " 15,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 14,\n",
       " 5,\n",
       " 6,\n",
       " 14,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 15,\n",
       " 2,\n",
       " 12,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 12,\n",
       " 6,\n",
       " 14,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 14,\n",
       " 6,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 12,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 12,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "Y_train = torch.LongTensor(Y_train)\n",
    "Y_test = torch.LongTensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.Adam(model.parameters(),lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MBTI_NN(\n",
       "  (hidden1): Linear(in_features=4096, out_features=16, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): LogSoftmax()\n",
       ")>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 and loss 2.777109146118164\n",
      "epoch: 2 and loss 2.777109146118164\n",
      "epoch: 4 and loss 2.777109146118164\n",
      "epoch: 6 and loss 2.777109146118164\n",
      "epoch: 8 and loss 2.777109146118164\n",
      "epoch: 10 and loss 2.777109146118164\n",
      "epoch: 12 and loss 2.777109146118164\n",
      "epoch: 14 and loss 2.777109146118164\n",
      "epoch: 16 and loss 2.777109146118164\n",
      "epoch: 18 and loss 2.777109146118164\n",
      "epoch: 20 and loss 2.777109146118164\n",
      "epoch: 22 and loss 2.777109146118164\n",
      "epoch: 24 and loss 2.777109146118164\n",
      "epoch: 26 and loss 2.777109146118164\n",
      "epoch: 28 and loss 2.777109146118164\n",
      "epoch: 30 and loss 2.777109146118164\n",
      "epoch: 32 and loss 2.777109146118164\n",
      "epoch: 34 and loss 2.777109146118164\n",
      "epoch: 36 and loss 2.777109146118164\n",
      "epoch: 38 and loss 2.777109146118164\n",
      "epoch: 40 and loss 2.777109146118164\n",
      "epoch: 42 and loss 2.777109146118164\n",
      "epoch: 44 and loss 2.777109146118164\n",
      "epoch: 46 and loss 2.777109146118164\n",
      "epoch: 48 and loss 2.777109146118164\n",
      "epoch: 50 and loss 2.777109146118164\n",
      "epoch: 52 and loss 2.777109146118164\n",
      "epoch: 54 and loss 2.777109146118164\n",
      "epoch: 56 and loss 2.777109146118164\n",
      "epoch: 58 and loss 2.777109146118164\n",
      "epoch: 60 and loss 2.777109146118164\n",
      "epoch: 62 and loss 2.777109146118164\n",
      "epoch: 64 and loss 2.777109146118164\n",
      "epoch: 66 and loss 2.777109146118164\n",
      "epoch: 68 and loss 2.777109146118164\n",
      "epoch: 70 and loss 2.777109146118164\n",
      "epoch: 72 and loss 2.777109146118164\n",
      "epoch: 74 and loss 2.777109146118164\n",
      "epoch: 76 and loss 2.777109146118164\n",
      "epoch: 78 and loss 2.777109146118164\n",
      "epoch: 80 and loss 2.777109146118164\n",
      "epoch: 82 and loss 2.777109146118164\n",
      "epoch: 84 and loss 2.777109146118164\n",
      "epoch: 86 and loss 2.777109146118164\n",
      "epoch: 88 and loss 2.777109146118164\n",
      "epoch: 90 and loss 2.777109146118164\n",
      "epoch: 92 and loss 2.777109146118164\n",
      "epoch: 94 and loss 2.777109146118164\n",
      "epoch: 96 and loss 2.777109146118164\n",
      "epoch: 98 and loss 2.777109146118164\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = criterion(y_pred, Y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if i%2==0:\n",
    "        print(f\"epoch: {i} and loss {loss}\")\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf  = LogisticRegression(random_state=42,max_iter=1000,penalty='l2',C=0.1)\n",
    "clf.fit(embedding, label_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3254054054054054"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "m = XGBClassifier(\n",
    "    max_depth=2,\n",
    "    gamma=2,\n",
    "    eta=0.8,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, eta=0.8, gamma=2,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.800000012, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0.5,\n",
       "              reg_lambda=0.5, scale_pos_weight=None, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 23.46%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = m.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033 7215\n"
     ]
    }
   ],
   "source": [
    "Y_mask = []\n",
    "c_1 = 0\n",
    "c_0 = 0\n",
    "for i in label_enc:\n",
    "    if i >=0 and i<=7:\n",
    "        Y_mask.append(1)\n",
    "        c_1+=1\n",
    "    else:\n",
    "        Y_mask.append(2)\n",
    "        c_0+=1\n",
    "print(c_0,c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mask, X_test_mask, Y_train_mask, Y_test_mask = train_test_split(embedding,Y_mask,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf  = LogisticRegression(random_state=42,max_iter=1000,penalty='l2',C=0.1)\n",
    "clf.fit(X_train_mask, Y_train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6625086625086625"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_mask, Y_test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, eta=0.8, gamma=2,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.800000012, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0.5,\n",
       "              reg_lambda=0.5, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "              validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = XGBClassifier(\n",
    "    max_depth=2,\n",
    "    gamma=2,\n",
    "    eta=0.8,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5\n",
    ")\n",
    "m.fit(X_train_mask, Y_train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.70%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = m.predict(X_test_mask)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(Y_test_mask, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mask = torch.FloatTensor(X_train_mask)\n",
    "X_test_mask = torch.FloatTensor(X_test_mask)\n",
    "Y_train_mask = torch.LongTensor(Y_train_mask)\n",
    "Y_test_mask = torch.LongTensor(Y_test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_sm, y_sm = smote.fit_resample(embedding, Y_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mask, X_test_mask, Y_train_mask, Y_test_mask = train_test_split(X_sm,y_sm,test_size=0.2,random_state=42)\n",
    "X_train_mask = torch.FloatTensor(X_train_mask)\n",
    "X_test_mask = torch.FloatTensor(X_test_mask)\n",
    "Y_train_mask = torch.LongTensor(Y_train_mask)\n",
    "Y_test_mask = torch.LongTensor(Y_test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf  = LogisticRegression(random_state=42,max_iter=10000,penalty='l2',C=0.1)\n",
    "clf.fit(X_train_mask, Y_train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6625086625086625"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_mask, Y_test_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
